{
 "cells": [
  {
   "source": [
    "# This jupyter notebook scrapes unstructured data from the website \"The Political Graveyard\" (www.thepoliticalgraveyard.com) into an excel spreadsheet\n",
    "## The historical dataset that is generates is about all clergy politicians born in the United States by their birth state and county as well as lived state."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### You will need to download chromedriver to exceute this notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "# save the path to chromedriver here\n",
    "EXE_PATH = r'C:\\Users\\nikhi\\Desktop\\Summer 2020\\RA Work\\Scraping\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "# list of all possible clergy positions\n",
    "clergy_posts=['Pastor', 'Chaplain', 'chaplain', 'pastor','Priest','priest', 'Pastor', 'Minister', 'minister', 'missionary', 'Missionary', 'Rabbi', 'rabbi', 'bishop', 'Bishop', 'Salvation Army officer' , 'cleric', 'Cleric', 'Preacher', 'preacher', 'preacher', 'Clergyman', 'clergyman']\n",
    "\n",
    "# a dictionary for data entries\n",
    "clergy_pol = {}\n",
    "\n",
    "# counter for dictionary entry\n",
    "count = 0\n",
    "\n",
    "url = \"http://politicalgraveyard.com/occ/clergy.html\"\n",
    "    \n",
    "# open the link to the webpage\n",
    "browser.get(url)\n",
    "browser.implicitly_wait(5)\n",
    "\n",
    "# read HTML content of the webpage\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# get all the links in that reigion's webpage\n",
    "link = soup.findAll('a')\n",
    "\n",
    "# a list of all the links for various states for that religion \n",
    "state_links=[]\n",
    "\n",
    "# loop over all the links in that religion's webpage\n",
    "# the strategy is to keep opening the 'geo' links until 'The Political Grveyard' link appears \n",
    "# when the loop breaks\n",
    "\n",
    "for each in link:\n",
    "    if('The Political Graveyard' in each.text):\n",
    "        break\n",
    "    elif(('geo' in each.get('href')) == True):\n",
    "        # get the link for the states and append to the list\n",
    "        state_links.append(each.get('href'))\n",
    "\n",
    "# loop over all the 'geo' links in that religion's webpage\n",
    "# the strategy is to keep opening the 'geo' links and extract \n",
    "# state name and politician's name from the webpage\n",
    "\n",
    "for st in state_links:\n",
    "    try:\n",
    "        html = requests.get(st)\n",
    "        soup = BeautifulSoup(html.content)\n",
    "\n",
    "        # identifier for state name\n",
    "        a = 'Politicians in'\n",
    "\n",
    "        # loop over all the <\\p> tags to identify state's name in each webpage\n",
    "        for x in (soup.find_all('p')):\n",
    "                state = str(x)[str(x).find(a)+15:str(x).find('</p>')]\n",
    "\n",
    "        # open the state link\n",
    "        browser.get(st)\n",
    "        browser.implicitly_wait(5)\n",
    "            \n",
    "        html = urlopen(st)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        #  save the html content as a string\n",
    "        string = str(soup)\n",
    "\n",
    "        # find all the bold texts in the page\n",
    "        name = soup.findAll('b')\n",
    "\n",
    "        # flag to identify whether the bold text is a new name or modified version of the old name\n",
    "        # Here I use the fact that new names appear only after the religion name is mentioned in the bold text\n",
    "        flag = 1\n",
    "            \n",
    "        # list of all politician names found on the religion by state webpage\n",
    "        pol_list=[]\n",
    "\n",
    "        # loop over all the bold text tags\n",
    "        for each in name:\n",
    "\n",
    "            # if the text contains the word 'Political', it means that all the names have been scraped\n",
    "            if('Political' in each.text):\n",
    "                break\n",
    "\n",
    "            # if next bold tag is a new name, flag = 1\n",
    "            if flag==1:\n",
    "                if (any(x in clergy_posts for x in each.text.split())):\n",
    "                    continue\n",
    "\n",
    "                else: \n",
    "                    if ('(' in each.text):\n",
    "                        # if text contains info on birth/death year in (), remove that and save the data\n",
    "                        pol_list.append(each.text[0:each.text.find('(')])\n",
    "                    else: \n",
    "                        pol_list.append(each.text)\n",
    "\n",
    "                    # next tag will not be a new name\n",
    "                    flag = 0\n",
    "\n",
    "            elif (any(x in clergy_posts for x in each.text.split())):\n",
    "                # if the bold tag contains a religion name, the next bold tag will have a new name\n",
    "                flag = 1\n",
    "        # locate the positions of all the politician names in the text of the html content\n",
    "        locate=[]\n",
    "        for each in pol_list:\n",
    "            locate.append(string.find(each))\n",
    "        \n",
    "        l = len(locate)\n",
    "        for x in np.arange(l):\n",
    "            if x<=l-2:\n",
    "                soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                for each in (soup.find_all('a')):\n",
    "                    if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                        temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                        temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                        temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                        temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                        # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                        dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                        count +=1\n",
    "\n",
    "            elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                    if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                        temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                        temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                        temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                        temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                        # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                        dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                        count +=1\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary into a dataframe\n",
    "df_clergy_pol=pd.DataFrame.from_dict(clergy_pol).T\n",
    "\n",
    "# save the dataframe into an excel file\n",
    "df_clergy_pol.to_excel('clergy_pol.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an excel file that abbreviations for US state names into dataframe\n",
    "df_st = pd.read_excel('state_name_abb.xlsx')\n",
    "df_st['ABB'] = df_st['ABB'].str.replace(' ','')\n",
    "for abb in df_clergy_pol['born_state']:\n",
    "    # for each abbreviation in the clergy names dataframe\n",
    "    flag = 0\n",
    "    for each in df_st['ABB']:\n",
    "        # for each state in US\n",
    "        if str(each) in str(abb):\n",
    "            # if part of the state name is in an actual state name\n",
    "            # replace the state name (abbreviated) to full state name\n",
    "            temp = df_st.loc[df_st['ABB']==each].iloc[0,0]\n",
    "            df_clergy_pol['born_state'] = df_clergy_pol['born_state'].replace(abb, temp)\n",
    "            flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe into an excel file\n",
    "df_clergy_pol.to_excel('clergy_pol_state.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitc93c2ab14bb54a9e898317ef2f21e286",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}