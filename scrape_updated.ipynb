{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scraping Code: \n",
    "## Part of RA work for Prof. Martin Fiszbein at Boston University (2020)\n",
    "## See http://politicalgraveyard.com/index.html \n",
    "\n",
    "## Name: Nikhil Kumar\n",
    "\n",
    "### 1. What does the code do? \n",
    "To scrape the name of politicians along with their birht county, state and religious denomiation from the website above.\n",
    "\n",
    "### 2. How can you use it? \n",
    "In the 'EXE_PATH', you need to add the path of your own computer and then you can click on 'Run'. A new browser will open in your Computer containing the URL. The webiste is poorly organized in its HTML construction and I have used the pattern of rendering of data for retreiving the data that is required for analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required functions and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an excel sheet for politicians name by birth state and county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "#EXE_PATH = r\"F:/Winter 2020/RA Martin/Scraping_Summer_2020/chromedriver.exe\"\n",
    "#browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "curr = \"http://politicalgraveyard.com/geo/plindex.html\"\n",
    "\n",
    "# open the webpage with all the links to different states\n",
    "#browser.get(curr)\n",
    "#browser.implicitly_wait(1)\n",
    "\n",
    "html = urlopen(curr)\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# find all the links in the webpage\n",
    "link_one = soup.findAll('a')\n",
    "\n",
    "# counter for dictionary entries\n",
    "count = 0\n",
    "\n",
    "# dictionary for storage\n",
    "dic_pol_name_state_county={}\n",
    "\n",
    "for state_link in link_one:\n",
    "# loop over the links in the webpage\n",
    "# the strategy is to keep opening the 'geo' links until 'The Political Grveyard' link appears \n",
    "# then the loop breaks\n",
    "\n",
    "    if('The Political Graveyard' in state_link.text):\n",
    "        break\n",
    "    elif(('geo' in state_link.get('href')) == True):\n",
    "\n",
    "        # find the state name from the link text\n",
    "        state = state_link.get_text().replace('\\n', ' ')\n",
    "        #print(\"STATE\", state)\n",
    "\n",
    "        if state=='Wyoming':\n",
    "        # links in wyoming do not work on the website\n",
    "            continue\n",
    "\n",
    "        # open the link for each state    \n",
    "        #browser.get(state_link.get('href'))\n",
    "        #browser.implicitly_wait(1)\n",
    "\n",
    "        html = urlopen(state_link.get('href'))\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        # find the links in each webpages for each state\n",
    "        link_two = soup.findAll('a')\n",
    "\n",
    "        # loop over all the links in that state's webpage\n",
    "        # the strategy is to keep opening the 'born' links until 'The Political Grveyard' link appears \n",
    "        # when the loop breaks\n",
    "\n",
    "        for born_link in link_two:\n",
    "            if('The Political Graveyard' in born_link.text):\n",
    "                break\n",
    "            elif(('born' in born_link.get('href')) == True):\n",
    "                \n",
    "                # open the link for each county\n",
    "                #browser.get(born_link.get('href'))\n",
    "                #browser.implicitly_wait(1)\n",
    "                \n",
    "                html = requests.get(born_link.get('href'))\n",
    "                soup = BeautifulSoup(html.content)\n",
    "\n",
    "                # create an identifier for county\n",
    "                a = ['County', 'county', 'Borough', 'borough', 'census area', 'city', 'City','municipality', 'Miscellaneous', 'Parish', 'Municipio']\n",
    "\n",
    "                # Now we find the name of the county\n",
    "                for x in (soup.find('p')):\n",
    "                    # check if any of the above county-identifier words are present in the county name tag\n",
    "                    if (any(identifier in x for identifier in a)):\n",
    "                        county = str(x).replace('\\n','')\n",
    "                        print(county)\n",
    "\n",
    "                html = urlopen(born_link.get('href'))\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "                link_three = soup.findAll('a')\n",
    "\n",
    "                # now loop over all the 'bio' links in the county's webpage\n",
    "                # and extract all politician's name for that county\n",
    "                # till 'The political Graveyard' link appears\n",
    "                # when the loop breaks\n",
    "\n",
    "                for bio_link in link_three:\n",
    "                    if('The Political Graveyard' in bio_link.text):\n",
    "                        break\n",
    "                    elif(('bio' in bio_link.get('href')) == True):\n",
    "                        # get the text the link which contains politicians names\n",
    "                        pol_name = bio_link.get_text().replace('\\n', ' ')\n",
    "\n",
    "                        # save the data as a dictionary entry\n",
    "                        dic_pol_name_state_county[count]={'state':state, 'county':county, 'pol_name':pol_name}\n",
    "                        \n",
    "\n",
    "                        # increase the counter\n",
    "                        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary into a dataframe\n",
    "df_pol_name_state_county=pd.DataFrame.from_dict(dic_pol_name_state_county).T\n",
    "\n",
    "# save the data frame into n excel sheet\n",
    "df_pol_name_state_county.to_excel('data_born_pol.xlsx',engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitc93c2ab14bb54a9e898317ef2f21e286",
   "display_name": "Python 3.7.7 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}