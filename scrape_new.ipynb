{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "EXE_PATH = r'C:\\Users\\nikhi\\Desktop\\Summer 2020\\RA Work\\Scraping\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "curr=\"http://politicalgraveyard.com/geo/CA/methodist.html\"\n",
    "html = requests.get(curr)\n",
    "soup = BeautifulSoup(html.content)\n",
    "string = str(soup)\n",
    "browser.get(curr)\n",
    "browser.implicitly_wait(1)\n",
    "\n",
    "count = 0\n",
    "flag = 1\n",
    "pol_list=[]\n",
    "\n",
    "for each in (soup.find_all('b')):\n",
    "\n",
    "     # if the text contains the word 'Political', it means that all the names have been scraped\n",
    "                if('Political' in each.text):\n",
    "                    break\n",
    "\n",
    "                # if next bold tag is a new name, flag = 1\n",
    "                if flag==1:\n",
    "                    if ('(' in each.text):\n",
    "                        # if text contains info on birth/death year in (), remove that and save the data\n",
    "                        pol_list.append(each.text[0:each.text.find('(')])\n",
    "                    else: \n",
    "                        pol_list.append(each.text)\n",
    "                    # increase the counter\n",
    "                    count += 1\n",
    "\n",
    "                    # next tag will not be a new name\n",
    "                    flag = 0\n",
    "\n",
    "                elif ('Methodist' in each.text):\n",
    "                    # if the bold tag contains a religion name, the next bold tag will have a new name\n",
    "                    flag = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2424, 3241, 6275, 8516, 10071, 11445, 13431, 15023, 18067, 21424, 22544, 26159, 29623, 33098, 35307, 37366, 40753, 42513, 44333, 48986, 50499, 53456, 55773, 56834, 59406, 60539, 62644, 64534, 66920, 69498, 71569, 72849, 75409, 77098, 78703, 80746, 82718, 87874, 89766, 93312, 95558, 97918, 100742, 104031, 106632, 108254, 110296, 112230, 113485, 114875, 116142, 118099, 120415, 123070, 125679, 128128, 135405, 138056, 139709, 142191, 144720, 146833, 148439, 149797, 151650]\n"
    }
   ],
   "source": [
    "# locate all the politician names in the string form of \n",
    "locate=[]\n",
    "for each in pol_list:\n",
    "    locate.append(string.find(each))\n",
    "print(locate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prentiss County\nMiss.\nHumboldt County\nCalif.\nSaratoga County\nN.Y.\nBremer County\nIowa\nSanta Clara County\nCalif.\nSan Joaquin County\nCalif.\nPershing County\nNev.\nToombs County\nGa.\nPhiladelphia County\nPa.\nRobertson County\nTex.\nImperial County\nCalif.\nWood County\nTex.\nParke County\nInd.\nDelaware County\nIowa\nBowman County\nN.Dak.\nBrown County\nWis.\nOntario\n\nEssex County\nN.J.; Santa Paula\nLouisa County\nVa.\nCherokee County\nKan.\nWyandotte County\nKan.\nSuffolk County\nMass.\nMorgan County\nIll.\nMcIntosh County\nOkla. Democrat.\nJohnson County\nIowa\nLos Angeles County\nCalif.\nMadison County\nN.Y.\nWestmoreland County\nPa.\nNicholas County\nW.Va.\nReeves County\nTex.\nCaddo Parish\nLa.\nDodge County\nWis.\nFresno County\nCalif. Born in Coalinga\nSan Diego County\nCalif.\nSouth Korea\n\nAlameda County\nCalif.; Piedmont\nBoyle County\nKy. Born in Danville\nCook County\nIll.\nLos Angeles County\nCalif.\nMarion County\nIll. Born in Salem\nTulare County\nCalif.; Fresno\nSacramento County\nCalif. Born in Sacramento\nSanta Clara County\nCalif. Born in San Jose\nWeber County\nUtah\nClark County\nMo.; Kahoka\nLos Angeles County\nCalif. Born in Whittier\nQuebec\nabout 1830.\nMitchell County\nIowa\nMarion County\nInd.\nRichmond\nVa.\nNemaha County\nNeb.\nWilliamson County\nTenn.\nNew Castle County\nDel.\nRoss County\nOhio\nKane County\nIll.\nMerced County\nCalif.\nYellowstone County\nMont.\nDelaware County\nN.Y.\nWinnebago County\nWis.\nVentura County\nCalif. Born in Ventura\nOntario\nabout 1853. Republican.\nWexford County\nMich.\nChickasaw County\nIowa\n"
    }
   ],
   "source": [
    "county={}\n",
    "count = 0\n",
    "l = len(locate)\n",
    "for x in np.arange(l):\n",
    "    if x<=l-2:\n",
    "        soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "        for each in (soup.find_all('a')):\n",
    "            if 'born' in each.get('href'):\n",
    "                print(each.get_text().replace('\\n', ' '))\n",
    "                temp1 = string[locate[x]:locate[x+1]].find(each.get_text())\n",
    "                temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5\n",
    "                temp3 = string[locate[x]:locate[x+1]].find('<a',temp2)\n",
    "                temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ')\n",
    "\n",
    "                print(temp4[:temp4.rfind(',')])\n",
    "\n",
    "    elif x ==l-1:\n",
    "        soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "        for each in (soup.find_all('a')):\n",
    "            if 'born' in each.get('href'):\n",
    "                print(each.get_text().replace('\\n', ' '))\n",
    "                temp1 = string[locate[x]:].find(each.get_text())\n",
    "                temp2 = string[locate[x]:].find('/a>',temp1)+5\n",
    "                temp3 = string[locate[x]:].find('<a',temp2)\n",
    "                temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                print(temp4[:temp4.rfind(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "EXE_PATH = r'C:\\Users\\nikhi\\Desktop\\Summer 2020\\RA Work\\Scraping\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "# the following religious denominations have their data arranged statewise\n",
    "religion1 = ['Methodist', 'Catholic', 'Presbyterian' , 'Episcopalian', 'Baptist', 'Congregationalist', 'Jewish', 'Lutheran', 'Unitarian', 'Protestant', 'Christian', 'Mormon', 'Disciples of Christ', 'Quaker', 'Christian Reformed']\n",
    "\n",
    "# the following religious denominations have their data all in the same webpage\n",
    "religion2=['Church of Christ', 'Eastern Orthodox', 'Christian Scientist', 'Pentecostal', 'Brethren', 'Atheist or agnostic', 'Muslim', 'Swedenborgian', 'Adventist', 'Nazarene', 'Mennonite', 'Spiritualist', 'Buddhist' , 'Puritan' , 'Deist', 'Scientologist', 'Hindu']\n",
    "\n",
    "# a dictionary for data entries\n",
    "dic_pol_name_state_rel = {}\n",
    "\n",
    "# counter for dictionary entry\n",
    "count = 0\n",
    "\n",
    "# loop over each religion\n",
    "for rel in religion1:\n",
    "\n",
    "    url = \"http://politicalgraveyard.com/index.html#PE\"\n",
    "    \n",
    "    # opne the link to the webpage\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(5)\n",
    "\n",
    "    # click on the link whose text contains name of the religion\n",
    "    rel_link = browser.find_element_by_link_text(rel).click()\n",
    "\n",
    "    # get the URL of the link\n",
    "    rel_url = browser.current_url\n",
    "\n",
    "    html = urlopen(rel_url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # get all the links in that reigion's webpage\n",
    "    link = soup.findAll('a')\n",
    "\n",
    "    # a list of all the links for various states for that religion \n",
    "    state_links=[]\n",
    "\n",
    "    # loop over all the links in that religion's webpage\n",
    "    # the strategy is to keep opening the 'geo' links until 'The Political Grveyard' link appears \n",
    "    # when the loop breaks\n",
    "\n",
    "    for each in link:\n",
    "        if('The Political Graveyard' in each.text):\n",
    "            break\n",
    "        elif(('geo' in each.get('href')) == True):\n",
    "            # get the link for the states and append to the list\n",
    "            state_links.append(each.get('href'))\n",
    "\n",
    "    # loop over all the 'geo' links in that religion's webpage\n",
    "    # the strategy is to keep opening the 'geo' links and extract \n",
    "    # state name and politician's name from the webpage\n",
    "\n",
    "    for st in state_links:\n",
    "        try:\n",
    "            html = requests.get(st)\n",
    "            soup = BeautifulSoup(html.content)\n",
    "\n",
    "            # identifier for state name\n",
    "            a = 'Politicians in'\n",
    "\n",
    "            # loop over all the <\\p> tags to identify state's name in each webpage\n",
    "            for x in (soup.find_all('p')):\n",
    "                if rel in ['Episcopalian', 'Congregationalist', 'Jewish', 'Protestant', 'Mormon', 'Quaker', 'Christian Reformed']:\n",
    "                    # in these religions, the text contains additional religion information that we do not want in the state name\n",
    "                    # we substring accordingly\n",
    "                    state = str(x)[str(x).find(a)+15:str(x).find('<br/>',str(x).find(a)+15)]\n",
    "                else: \n",
    "                    state = str(x)[str(x).find(a)+15:str(x).find('</p>')]\n",
    "\n",
    "            # open the state link\n",
    "            browser.get(st)\n",
    "            browser.implicitly_wait(5)\n",
    "            \n",
    "            html = urlopen(st)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            #  save the html content as a string\n",
    "            string = str(soup)\n",
    "\n",
    "            # find all the bold texts in the page\n",
    "            name = soup.findAll('b')\n",
    "\n",
    "            # flag to identify whether the bold text is a new name or modified version of the old name\n",
    "            # Here I use the fact that new names appear only after the religion name is mentioned in the bold text\n",
    "            flag = 1\n",
    "            \n",
    "            # list of all politician names found on the religion by state webpage\n",
    "            pol_list=[]\n",
    "\n",
    "            # loop over all the bold text tags\n",
    "            for each in name:\n",
    "\n",
    "                # if the text contains the word 'Political', it means that all the names have been scraped\n",
    "                if('Political' in each.text):\n",
    "                    break\n",
    "\n",
    "                # if next bold tag is a new name, flag = 1\n",
    "                if flag==1:\n",
    "                    if ('(' in each.text):\n",
    "                        # if text contains info on birth/death year in (), remove that and save the data\n",
    "                        pol_list.append(each.text[0:each.text.find('(')])\n",
    "                    else: \n",
    "                        pol_list.append(each.text)\n",
    "\n",
    "                    # next tag will not be a new name\n",
    "                    flag = 0\n",
    "\n",
    "                elif (rel in each.text):\n",
    "                    # if the bold tag contains a religion name, the next bold tag will have a new name\n",
    "                    flag = 1\n",
    "\n",
    "            # locate the positions of all the politician names in the text of the html content\n",
    "            locate=[]\n",
    "            for each in pol_list:\n",
    "                locate.append(string.find(each))\n",
    "\n",
    "            # find the length of 'locate' list\n",
    "            l = len(locate)\n",
    "            for x in np.arange(l):\n",
    "                if x<=l-2:\n",
    "                    # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                    soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')): # for each link in the substring\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "\n",
    "                elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                    # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                    soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "    \n",
    "            # Some state's have their counties clubbed alphabetically A-C, D-F etc...\n",
    "\n",
    "            html = urlopen(st)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            # find all the links in that state's webpage\n",
    "            link = soup.findAll('a')\n",
    "\n",
    "            # create a list of links containing the links to all the clubbed pages\n",
    "            ord = []\n",
    "\n",
    "            # if next bold tag is a new name, flag = 1\n",
    "            flag = 0\n",
    "\n",
    "            for each in link:\n",
    "                # based on the link texts, identify the useful links that have the names of politians \n",
    "                if (('to' in each.get_text()) & ('zz' in each.get_text()) & ('geo' in each.get('href'))):\n",
    "                    # save the links into the list\n",
    "                    ord.append(each.get(\"href\"))\n",
    "                    flag = 1\n",
    "\n",
    "            # if such kind of clubbing exists on ghe webpage, then run this part of the code\n",
    "            if (flag == 1):\n",
    "                # loop over all the links stored in the list above\n",
    "                for curr in ord:\n",
    "\n",
    "                    # open each link\n",
    "                    html = urlopen(curr)\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                    #  save the html content as a string\n",
    "                    string = str(soup)\n",
    "\n",
    "                    # list of all politician names found on the religion by state webpage\n",
    "                    pol_list=[]\n",
    "\n",
    "                    # find all the bold tags\n",
    "                    name=soup.findAll('b')\n",
    "\n",
    "                    # for each bold tag, check if it has a new name\n",
    "                    for each in name:\n",
    "                        \n",
    "                        # if the word 'Political' appears in a new bold tag, it means all the new names have been scraped\n",
    "                        # so break\n",
    "                        if('Political' in each.text):\n",
    "                            break\n",
    "\n",
    "                        # if the next bold tag has a new name\n",
    "                        if flag==1:\n",
    "                            if ('(' in each.text):\n",
    "                                # if text contains info on birth/death year in (), remove that and save the data\n",
    "                                pol_list.append(each.text[0:each.text.find('(')])\n",
    "                            else: \n",
    "                                pol_list.append(each.text)\n",
    "\n",
    "\n",
    "                            # the next bold tag does not contain a new name\n",
    "                            flag = 0\n",
    "\n",
    "                        elif (rel in each.text):\n",
    "                            # if this tag has a religion name, the next bold tag has a new name\n",
    "                            flag = 1\n",
    "                            \n",
    "                    # locate the positions of all the politician names in the text of the html content\n",
    "                    locate=[]\n",
    "                    for each in pol_list:\n",
    "                        locate.append(string.find(each))\n",
    "\n",
    "                    l = len(locate)\n",
    "                    for x in np.arange(l):\n",
    "                        if x<=l-2:\n",
    "                            soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                            for each in (soup.find_all('a')):\n",
    "                                if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                                    temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                                    temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                                    temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                                    temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                                    # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                                    dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                                    count +=1\n",
    "\n",
    "                        elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                            # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                            soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                            for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                                if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                                    temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                                    temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                                    temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                                    temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                                    # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                                    dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                                    count +=1\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "EXE_PATH = r'C:\\Users\\nikhi\\Desktop\\Summer 2020\\RA Work\\Scraping\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "# loop over each religion\n",
    "for rel in religion2:\n",
    "\n",
    "    url = \"http://politicalgraveyard.com/index.html#PE\"\n",
    "    \n",
    "    # open each URL in the webpage\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(5)\n",
    "\n",
    "    # click on the link containing the religion name in text\n",
    "    rel_link = browser.find_element_by_link_text(rel).click()\n",
    "    rel_url = browser.current_url\n",
    "\n",
    "    html = urlopen(rel_url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # find all the link in the religion webpage\n",
    "    link = soup.findAll('a')\n",
    "\n",
    "    try:\n",
    "            browser.get(rel_url)\n",
    "            browser.implicitly_wait(5)\n",
    "            \n",
    "            html = urlopen(rel_url)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            #  save the html content as a string\n",
    "            string = str(soup)\n",
    "\n",
    "            # list of all politician names found on the religion by state webpage\n",
    "            pol_list=[]\n",
    "\n",
    "            # find all the bold tags in the religion webpage\n",
    "            name = soup.findAll('b')\n",
    "\n",
    "            # flag to identify if the next bold tag contains a new name\n",
    "            flag = 1\n",
    "            \n",
    "            # loop over each bold tag in the webpage\n",
    "            for each in name:\n",
    "\n",
    "                # if 'political' appears in the bold tag, it means all the names have been scraped\n",
    "                if('Political' in each.text):\n",
    "                    break\n",
    "\n",
    "                # if the next tag contains a new name, store the name\n",
    "                if flag==1:\n",
    "                    if ('(' in each.text):\n",
    "                        # if name has info on birth/death year, then remove that info by substring accordingly\n",
    "                        pol_list.append(each.text[0:each.text.find('(')])\n",
    "                    else: \n",
    "                        pol_list.append(each.text)\n",
    "\n",
    "                    # next bold tage is not a new name\n",
    "                    flag = 0\n",
    "\n",
    "                elif (rel in each.text):\n",
    "                    # if this tag has religion name, next bold tag has a new name\n",
    "                    flag = 1\n",
    "\n",
    "            #locate the positions of all the politician names in the text of the html content\n",
    "            locate=[]\n",
    "            for each in pol_list:\n",
    "                locate.append(string.find(each))\n",
    "        \n",
    "            l = len(locate)\n",
    "            for x in np.arange(l):\n",
    "                if x<=l-2:\n",
    "                    soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')):\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "\n",
    "                elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                    # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                    soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary into a dataframe\n",
    "df_pol_name_state_rel=pd.DataFrame.from_dict(dic_pol_name_state_rel).T\n",
    "\n",
    "# save the dataframe into an excel file\n",
    "df_pol_name_state_rel.to_excel('data_pol_name_rel.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_st = pd.read_excel('state_name_abb.xlsx')\n",
    "df_st['ABB'] = df_st['ABB'].str.replace(' ','')\n",
    "df_pol_name_state_rel = pd.read_excel('data_pol_name_rel2.xlsx')\n",
    "for abb in df_pol_name_state_rel['born_state']:\n",
    "    flag = 0\n",
    "    for each in df_st['ABB']:\n",
    "        if str(each) in str(abb):\n",
    "            temp = df_st.loc[df_st['ABB']==each].iloc[0,0]\n",
    "            df_pol_name_state_rel['born_state'] = df_pol_name_state_rel['born_state'].replace(abb, temp)\n",
    "            flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe into an excel file\n",
    "df_pol_name_state_rel.to_excel('data_pol_name_rel_county_state_born.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitc93c2ab14bb54a9e898317ef2f21e286",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}