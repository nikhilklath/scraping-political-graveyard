{
 "cells": [
  {
   "source": [
    "# This jupyter notebook scrapes unstructured data from the website \"The Political Graveyard\" (www.thepoliticalgraveyard.com) into an excel spreadsheet\n",
    "## The historical dataset that is generates is about all religious politicians born in the United States by their religion, birth state and county as well as lived state.\n",
    "### You will need to download chromedriver to exceute this notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "\n",
    "# add the location for chromedriver her\n",
    "EXE_PATH = r'C:\\Users\\nikhi\\Desktop\\Summer 2020\\RA Work\\Scraping\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "# the following religious denominations have their data arranged statewise\n",
    "religion1 = ['Methodist', 'Catholic', 'Presbyterian' , 'Episcopalian', 'Baptist', 'Congregationalist', 'Jewish', 'Lutheran', 'Unitarian', 'Protestant', 'Christian', 'Mormon', 'Disciples of Christ', 'Quaker', 'Christian Reformed']\n",
    "\n",
    "# the following religious denominations have their data all in the same webpage\n",
    "religion2=['Church of Christ', 'Eastern Orthodox', 'Christian Scientist', 'Pentecostal', 'Brethren', 'Atheist or agnostic', 'Muslim', 'Swedenborgian', 'Adventist', 'Nazarene', 'Mennonite', 'Spiritualist', 'Buddhist' , 'Puritan' , 'Deist', 'Scientologist', 'Hindu']\n",
    "\n",
    "# a dictionary for data entries\n",
    "dic_pol_name_state_rel = {}\n",
    "\n",
    "# counter for dictionary entry\n",
    "count = 0\n",
    "\n",
    "# loop over each religion\n",
    "for rel in religion1:\n",
    "\n",
    "    url = \"http://politicalgraveyard.com/index.html#PE\"\n",
    "    \n",
    "    # opne the link to the webpage\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(5)\n",
    "\n",
    "    # click on the link whose text contains name of the religion\n",
    "    rel_link = browser.find_element_by_link_text(rel).click()\n",
    "\n",
    "    # get the URL of the link\n",
    "    rel_url = browser.current_url\n",
    "\n",
    "    html = urlopen(rel_url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # get all the links in that reigion's webpage\n",
    "    link = soup.findAll('a')\n",
    "\n",
    "    # a list of all the links for various states for that religion \n",
    "    state_links=[]\n",
    "\n",
    "    # loop over all the links in that religion's webpage\n",
    "    # the strategy is to keep opening the 'geo' links until 'The Political Grveyard' link appears \n",
    "    # when the loop breaks\n",
    "\n",
    "    for each in link:\n",
    "        if('The Political Graveyard' in each.text):\n",
    "            break\n",
    "        elif(('geo' in each.get('href')) == True):\n",
    "            # get the link for the states and append to the list\n",
    "            state_links.append(each.get('href'))\n",
    "\n",
    "    # loop over all the 'geo' links in that religion's webpage\n",
    "    # the strategy is to keep opening the 'geo' links and extract \n",
    "    # state name and politician's name from the webpage\n",
    "\n",
    "    for st in state_links:\n",
    "        try:\n",
    "            html = requests.get(st)\n",
    "            soup = BeautifulSoup(html.content)\n",
    "\n",
    "            # identifier for state name\n",
    "            a = 'Politicians in'\n",
    "\n",
    "            # loop over all the <\\p> tags to identify state's name in each webpage\n",
    "            for x in (soup.find_all('p')):\n",
    "                if rel in ['Episcopalian', 'Congregationalist', 'Jewish', 'Protestant', 'Mormon', 'Quaker', 'Christian Reformed']:\n",
    "                    # in these religions, the text contains additional religion information that we do not want in the state name\n",
    "                    # we substring accordingly\n",
    "                    state = str(x)[str(x).find(a)+15:str(x).find('<br/>',str(x).find(a)+15)]\n",
    "                else: \n",
    "                    state = str(x)[str(x).find(a)+15:str(x).find('</p>')]\n",
    "\n",
    "            # open the state link\n",
    "            browser.get(st)\n",
    "            browser.implicitly_wait(5)\n",
    "            \n",
    "            html = urlopen(st)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            #  save the html content as a string\n",
    "            string = str(soup)\n",
    "\n",
    "            # find all the bold texts in the page\n",
    "            name = soup.findAll('b')\n",
    "\n",
    "            # flag to identify whether the bold text is a new name or modified version of the old name\n",
    "            # Here I use the fact that new names appear only after the religion name is mentioned in the bold text\n",
    "            flag = 1\n",
    "            \n",
    "            # list of all politician names found on the religion by state webpage\n",
    "            pol_list=[]\n",
    "\n",
    "            # loop over all the bold text tags\n",
    "            for each in name:\n",
    "\n",
    "                # if the text contains the word 'Political', it means that all the names have been scraped\n",
    "                if('Political' in each.text):\n",
    "                    break\n",
    "\n",
    "                # if next bold tag is a new name, flag = 1\n",
    "                if flag==1:\n",
    "                    if ('(' in each.text):\n",
    "                        # if text contains info on birth/death year in (), remove that and save the data\n",
    "                        pol_list.append(each.text[0:each.text.find('(')])\n",
    "                    else: \n",
    "                        pol_list.append(each.text)\n",
    "\n",
    "                    # next tag will not be a new name\n",
    "                    flag = 0\n",
    "\n",
    "                elif (rel in each.text):\n",
    "                    # if the bold tag contains a religion name, the next bold tag will have a new name\n",
    "                    flag = 1\n",
    "\n",
    "            # locate the positions of all the politician names in the text of the html content\n",
    "            locate=[]\n",
    "            for each in pol_list:\n",
    "                locate.append(string.find(each))\n",
    "\n",
    "            # find the length of 'locate' list\n",
    "            l = len(locate)\n",
    "            for x in np.arange(l):\n",
    "                if x<=l-2:\n",
    "                    # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                    soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')): # for each link in the substring\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "\n",
    "                elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                    # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                    soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "    \n",
    "            # Some state's have their counties clubbed alphabetically A-C, D-F etc...\n",
    "\n",
    "            html = urlopen(st)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            # find all the links in that state's webpage\n",
    "            link = soup.findAll('a')\n",
    "\n",
    "            # create a list of links containing the links to all the clubbed pages\n",
    "            ord = []\n",
    "\n",
    "            # if next bold tag is a new name, flag = 1\n",
    "            flag = 0\n",
    "\n",
    "            for each in link:\n",
    "                # based on the link texts, identify the useful links that have the names of politians \n",
    "                if (('to' in each.get_text()) & ('zz' in each.get_text()) & ('geo' in each.get('href'))):\n",
    "                    # save the links into the list\n",
    "                    ord.append(each.get(\"href\"))\n",
    "                    flag = 1\n",
    "\n",
    "            # if such kind of clubbing exists on ghe webpage, then run this part of the code\n",
    "            if (flag == 1):\n",
    "                # loop over all the links stored in the list above\n",
    "                for curr in ord:\n",
    "\n",
    "                    # open each link\n",
    "                    html = urlopen(curr)\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                    #  save the html content as a string\n",
    "                    string = str(soup)\n",
    "\n",
    "                    # list of all politician names found on the religion by state webpage\n",
    "                    pol_list=[]\n",
    "\n",
    "                    # find all the bold tags\n",
    "                    name=soup.findAll('b')\n",
    "\n",
    "                    # for each bold tag, check if it has a new name\n",
    "                    for each in name:\n",
    "                        \n",
    "                        # if the word 'Political' appears in a new bold tag, it means all the new names have been scraped\n",
    "                        # so break\n",
    "                        if('Political' in each.text):\n",
    "                            break\n",
    "\n",
    "                        # if the next bold tag has a new name\n",
    "                        if flag==1:\n",
    "                            if ('(' in each.text):\n",
    "                                # if text contains info on birth/death year in (), remove that and save the data\n",
    "                                pol_list.append(each.text[0:each.text.find('(')])\n",
    "                            else: \n",
    "                                pol_list.append(each.text)\n",
    "\n",
    "\n",
    "                            # the next bold tag does not contain a new name\n",
    "                            flag = 0\n",
    "\n",
    "                        elif (rel in each.text):\n",
    "                            # if this tag has a religion name, the next bold tag has a new name\n",
    "                            flag = 1\n",
    "                            \n",
    "                    # locate the positions of all the politician names in the text of the html content\n",
    "                    locate=[]\n",
    "                    for each in pol_list:\n",
    "                        locate.append(string.find(each))\n",
    "\n",
    "                    l = len(locate)\n",
    "                    for x in np.arange(l):\n",
    "                        if x<=l-2:\n",
    "                            soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                            for each in (soup.find_all('a')):\n",
    "                                if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                                    temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                                    temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                                    temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                                    temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                                    # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                                    dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                                    count +=1\n",
    "\n",
    "                        elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                            # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                            soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                            for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                                if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                                    temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                                    temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                                    temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                                    temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                                    # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                                    dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                                    count +=1\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Selenium for web-scraping\n",
    "EXE_PATH = r'C:\\Users\\nikhi\\Desktop\\Summer 2020\\RA Work\\Scraping\\chromedriver.exe'\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)\n",
    "\n",
    "# loop over each religion\n",
    "for rel in religion2:\n",
    "\n",
    "    url = \"http://politicalgraveyard.com/index.html#PE\"\n",
    "    \n",
    "    # open each URL in the webpage\n",
    "    browser.get(url)\n",
    "    browser.implicitly_wait(5)\n",
    "\n",
    "    # click on the link containing the religion name in text\n",
    "    rel_link = browser.find_element_by_link_text(rel).click()\n",
    "    rel_url = browser.current_url\n",
    "\n",
    "    html = urlopen(rel_url)\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # find all the link in the religion webpage\n",
    "    link = soup.findAll('a')\n",
    "\n",
    "    try:\n",
    "            browser.get(rel_url)\n",
    "            browser.implicitly_wait(5)\n",
    "            \n",
    "            html = urlopen(rel_url)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            #  save the html content as a string\n",
    "            string = str(soup)\n",
    "\n",
    "            # list of all politician names found on the religion by state webpage\n",
    "            pol_list=[]\n",
    "\n",
    "            # find all the bold tags in the religion webpage\n",
    "            name = soup.findAll('b')\n",
    "\n",
    "            # flag to identify if the next bold tag contains a new name\n",
    "            flag = 1\n",
    "            \n",
    "            # loop over each bold tag in the webpage\n",
    "            for each in name:\n",
    "\n",
    "                # if 'political' appears in the bold tag, it means all the names have been scraped\n",
    "                if('Political' in each.text):\n",
    "                    break\n",
    "\n",
    "                # if the next tag contains a new name, store the name\n",
    "                if flag==1:\n",
    "                    if ('(' in each.text):\n",
    "                        # if name has info on birth/death year, then remove that info by substring accordingly\n",
    "                        pol_list.append(each.text[0:each.text.find('(')])\n",
    "                    else: \n",
    "                        pol_list.append(each.text)\n",
    "\n",
    "                    # next bold tage is not a new name\n",
    "                    flag = 0\n",
    "\n",
    "                elif (rel in each.text):\n",
    "                    # if this tag has religion name, next bold tag has a new name\n",
    "                    flag = 1\n",
    "\n",
    "            #locate the positions of all the politician names in the text of the html content\n",
    "            locate=[]\n",
    "            for each in pol_list:\n",
    "                locate.append(string.find(each))\n",
    "        \n",
    "            l = len(locate)\n",
    "            for x in np.arange(l):\n",
    "                if x<=l-2:\n",
    "                    soup = BeautifulSoup(string[locate[x]:locate[x+1]],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')):\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:locate[x+1]].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:locate[x+1]].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:locate[x+1]].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:locate[x+1]][temp2:temp3].replace('\\n', ' ') \n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "\n",
    "                elif x ==l-1: # if it is the last name in the list of politicians for that state\n",
    "                    # read the substring of 'string' as an html file through BeautifulSoup\n",
    "                    soup = BeautifulSoup(string[locate[x]:],features=\"html.parser\")\n",
    "                    for each in (soup.find_all('a')):  # for each link in the substring\n",
    "                        if 'born' in each.get('href'): # if the link is a 'born' link\n",
    "                            temp1 = string[locate[x]:].find(each.get_text()) # index of the link in the substring\n",
    "                            temp2 = string[locate[x]:].find('/a>',temp1)+5 # index of '/a>' after the link in the substring\n",
    "                            temp3 = string[locate[x]:].find('<a',temp2) # index of '<a' after '/a>' in the substring\n",
    "                            temp4 = string[locate[x]:][temp2:temp3].replace('\\n', ' ')\n",
    "                            # extract the born state name located between temp2 and temp3 by replacing the '\\n' with ' '\n",
    "\n",
    "                            dic_pol_name_state_rel[count] = {'lived_state':state, 'born_state':temp4[:temp4.rfind(',')], 'born_county':each.get_text().replace('\\n', ' '), 'religion':rel, 'pol_name':pol_list[x] }\n",
    "                            count +=1\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary into a dataframe\n",
    "df_pol_name_state_rel=pd.DataFrame.from_dict(dic_pol_name_state_rel).T\n",
    "\n",
    "# save the dataframe into an excel file\n",
    "df_pol_name_state_rel.to_excel('data_pol_name_rel.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open an excel file that abbreviations for US state names into dataframe\n",
    "df_st = pd.read_excel('state_name_abb.xlsx')\n",
    "df_st['ABB'] = df_st['ABB'].str.replace(' ','')\n",
    "df_pol_name_state_rel = pd.read_excel('data_pol_name_rel2.xlsx')\n",
    "for abb in df_pol_name_state_rel['born_state']:\n",
    "    # for each abbreviation in the clergy names dataframe\n",
    "    flag = 0\n",
    "    for each in df_st['ABB']:\n",
    "        if str(each) in str(abb):\n",
    "            # if part of the state name is in an actual state name\n",
    "            # replace the state name (abbreviated) to full state name\n",
    "            temp = df_st.loc[df_st['ABB']==each].iloc[0,0]\n",
    "            df_pol_name_state_rel['born_state'] = df_pol_name_state_rel['born_state'].replace(abb, temp)\n",
    "            flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe into an excel file\n",
    "df_pol_name_state_rel.to_excel('data_pol_name_rel_county_state_born.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitc93c2ab14bb54a9e898317ef2f21e286",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}